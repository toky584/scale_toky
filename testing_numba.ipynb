{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the sparse matrix.\n",
    "def sparse_split(filer):\n",
    "    import csv\n",
    "    with open(filer, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        map_user_to_idx = {}\n",
    "        map_idx_to_user = []\n",
    "        map_movie_to_idx = {}\n",
    "        map_idx_to_movie = []\n",
    "        data_by_user_train = []\n",
    "        data_by_user_test = []\n",
    "        data_by_movie_train = []\n",
    "        data_by_movie_test = []\n",
    "        data_by_user = []\n",
    "        data_by_movie = []\n",
    "        for row in reader:\n",
    "            user = row[0]\n",
    "            movie = row[1]\n",
    "            rating = row[2]\n",
    "            \n",
    "            if user =='userId':\n",
    "                continue\n",
    "            if user not in map_user_to_idx:\n",
    "                map_user_to_idx[user] = len(map_user_to_idx)\n",
    "                map_idx_to_user.append(user)\n",
    "                data_by_user_train.append([])\n",
    "                data_by_user_test.append([])\n",
    "                data_by_user.append([])\n",
    "            if movie not in map_movie_to_idx:\n",
    "                map_movie_to_idx[movie]=len(map_movie_to_idx)\n",
    "                map_idx_to_movie.append(movie)\n",
    "                data_by_movie_train.append([])\n",
    "                data_by_movie_test.append([])\n",
    "                data_by_movie.append([])\n",
    "            # flip a coin\n",
    "            data_by_user[map_user_to_idx[user]].append((map_movie_to_idx[movie],float(rating)))\n",
    "            data_by_movie[map_movie_to_idx[movie]].append((map_user_to_idx[user],float(rating)))            \n",
    "            flip = np.random.rand()\n",
    "            if flip < 0.05:\n",
    "                data_by_user_test[map_user_to_idx[user]].append((map_movie_to_idx[movie],float(rating)))\n",
    "                data_by_movie_test[map_movie_to_idx[movie]].append((map_user_to_idx[user],float(rating)))\n",
    "            else:\n",
    "                data_by_user_train[map_user_to_idx[user]].append((map_movie_to_idx[movie],float(rating)))\n",
    "                data_by_movie_train[map_movie_to_idx[movie]].append((map_user_to_idx[user],float(rating)))\n",
    "    return data_by_user, data_by_movie, data_by_user_train, data_by_movie_train, data_by_user_test, data_by_movie_test, map_idx_to_user, map_idx_to_movie, map_user_to_idx, map_movie_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaing the data to sparse matrix.\n",
    "path = 'ml-25m/ratings.csv'\n",
    "data_by_user, data_by_movie, data_by_user_train, data_by_movie_train, data_by_user_test, data_by_movie_test, map_idx_to_user, map_idx_to_movie, map_user_to_idx, map_movie_to_idx = sparse_split(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess of the dataset for numba.\n",
    "def user_transform(data_by_user):\n",
    "    list_for_users = [] #list for number of movies for each user.\n",
    "    list_for_ratings = [] #list for ratings and movies.\n",
    "    list_incrementing = [] #list for number of movies incrementing for each user.\n",
    "    summ = 0\n",
    "    for i in range(len(data_by_user)):\n",
    "        list_incrementing.append(summ)\n",
    "        list_for_users.append(len(data_by_user[i]))\n",
    "        summ+= len(data_by_user[i])\n",
    "        for (n,r) in data_by_user[i]:\n",
    "            rating = list((n,r))\n",
    "            list_for_ratings.append(rating)\n",
    "    list_incrementing.append(summ)\n",
    "    return np.array(list_for_users, dtype = np.int64), np.array(list_for_ratings, dtype = np.float64), np.array(list_incrementing, dtype = np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the function.\n",
    "lengths_user, data_array_user, lengths_user_incrementing = user_transform(data_by_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the function for the rest of the data.\n",
    "lengths_movie, data_array_movie, lengths_movie_incrementing = user_transform(data_by_movie)\n",
    "lengths_user_train, data_array_user_train, lengths_us_train_incrementing = user_transform(data_by_user_train)\n",
    "lengths_user_test, data_array_user_test, lengths_us_test_incrementing = user_transform(data_by_user_test)\n",
    "lengths_movie_train, data_array_movie_train, lengths_mv_train_incrementing = user_transform(data_by_movie_train)\n",
    "lengths_movie_test, data_array_movie_test, lengths_mv_test_incrementing = user_transform(data_by_movie_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for the updates for the data by user parallellyzed.\n",
    "from numba import njit, prange\n",
    "@njit(parallel = True)\n",
    "def updtating_biases_vector_user(user_biases, lengths_user, lengths_user_incre, movies_biases, user_vector, movie_vector, lamda, gamma, tau, data_by_user):\n",
    "  for m in prange (len(lengths_user)):\n",
    "    length1 = lengths_user_incre[m]\n",
    "    length2 = lengths_user_incre[m+1]\n",
    "    arr = np.zeros(length2 - length1).astype(np.float64)\n",
    "    for index_data in prange(length1, length2):\n",
    "      n = int(data_by_user[index_data, 0])\n",
    "      index = index_data - length1\n",
    "      arr[index] = lamda * (data_by_user[index_data,1] - movies_biases[n] - np.dot(user_vector[m,:], movie_vector[n,:]))\n",
    "    bias = np.sum(arr)\n",
    "    bias=bias/(lamda*len(arr) + gamma)\n",
    "    user_biases[m]=bias\n",
    "  \n",
    "  for m in prange(len(lengths_user)):\n",
    "    length1 = lengths_user[m]\n",
    "    length2 = lengths_user[m+1]\n",
    "    sum_vn = np.zeros((len(user_vector[m,:]),len(user_vector[m,:])))\n",
    "    sum_r_vn = np.zeros(len(user_vector[m,:]))\n",
    "    for indexi_data in range(length1, length2):\n",
    "      n = int(data_by_user[indexi_data, 0])\n",
    "      r = data_by_user[indexi_data, 1]\n",
    "      vn = movie_vector[n,:]\n",
    "      sum_vn += np.outer(vn,vn)\n",
    "      sum_r_vn += lamda*(r - user_biases[m] - movies_biases[n])*vn\n",
    "    sum_vn = lamda*(sum_vn) + tau*np.eye(len(vn))\n",
    "    user_vector[m,:] = np.linalg.solve(sum_vn, sum_r_vn)\n",
    "    \n",
    "  return user_biases, user_vector\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for the updates for the movies parallelyzed.\n",
    "@njit(parallel = True)\n",
    "def updtating_biases_vector_movie(user_biases, movies_biases, user_vector, movie_vector, lamda, gamma, tau, lengths_movie, lengths_movie_incre, data_by_movie):\n",
    "  for n in prange (len(lengths_movie)):\n",
    "    length1 = lengths_movie_incre[n]\n",
    "    length2 = lengths_movie_incre[n+1]\n",
    "    arr = np.zeros(length2 - length1).astype(np.float64)\n",
    "    for index_data in range(length1, length2):\n",
    "      m = int(data_by_movie[index_data, 0])\n",
    "      arr[index_data - length1] = lamda * (data_by_movie[index_data,1] - user_biases[m] - np.dot(user_vector[m,:], movie_vector[n,:]))\n",
    "    bias = np.sum(arr)\n",
    "    bias=bias/(lamda*len(arr) + gamma)\n",
    "    movies_biases[n]=bias\n",
    "\n",
    "  for n in prange(len(lengths_movie)):\n",
    "    length1 = lengths_movie_incre[n]\n",
    "    length2 = lengths_movie_incre[n+1]\n",
    "    sum_vn = np.zeros((len(movie_vector[n,:]),len(movie_vector[n,:])))\n",
    "    sum_r_vn = np.zeros(len(movie_vector[n,:]))\n",
    "    for indexi_data in range(length1, length2):\n",
    "      m = int(data_by_movie[indexi_data, 0])\n",
    "      r = data_by_movie[indexi_data, 1]\n",
    "      vn = user_vector[m,:]\n",
    "      sum_vn += np.outer(vn,vn)\n",
    "      sum_r_vn += lamda*(r - user_biases[m] - movies_biases[n])*vn\n",
    "    sum_vn = lamda*(sum_vn) + tau*np.eye(len(vn))\n",
    "    movie_vector[n,:] = np.linalg.solve(sum_vn, sum_r_vn)\n",
    "  return movies_biases, movie_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSE calculate parallelized\n",
    "@njit(parallel = True)\n",
    "def rmse_calculate(data, lengths, lengths_incre, biases_u, biases_v, vector_u, vector_v):\n",
    "  arr = np.zeros(len(lengths)).astype(np.float64)\n",
    "  for n in prange (len(lengths)):\n",
    "    length1 = lengths_incre[n]\n",
    "    length2 = lengths_incre[n+1]\n",
    "    arr1 = np.zeros(length2 - length1).astype(np.float64)\n",
    "    for indexa_data in prange(length1, length2):\n",
    "      m = int(data[indexa_data, 0])\n",
    "      r = data[indexa_data, 1]\n",
    "      arr1 [indexa_data - length1] = (r-biases_u[n]-biases_v[m]-np.dot(vector_u[n,:], vector_v[m,:]))**2\n",
    "    arr[n] = np.sum(arr1)\n",
    "  rmse = np.sum(arr)/lengths_incre[-1]\n",
    "  return np.sqrt(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(parallel=True)\n",
    "def loss_calculate(data, lengths, lengths_incre, biases_u, biases_v, vector_u, vector_v, lamda, gamma, tau):\n",
    "    arr = np.zeros(len(lengths), dtype=np.float64)\n",
    "    \n",
    "    for m in prange(len(lengths)):\n",
    "        length1 = lengths_incre[m]\n",
    "        length2 = lengths_incre[m + 1]\n",
    "        # Slice the relevant portion of the data for user\n",
    "        data_slice = data[length1:length2]\n",
    "        loss_sum = 0.0\n",
    "        \n",
    "        for indexb_data in range(length2 - length1):\n",
    "            n = int(data_slice[indexb_data, 0])\n",
    "            r = data_slice[indexb_data, 1] \n",
    "            um = vector_u[m, :]\n",
    "            vn = vector_v[n, :]\n",
    "            bm = biases_u[m]\n",
    "            bn = biases_v[n]\n",
    "            prediction_error = r - np.dot(um, vn) - bm - bn\n",
    "            loss = (-lamda / 2) * (prediction_error ** 2)\n",
    "            regularization = (-gamma / 2) * (bm ** 2 + bn ** 2) - (tau / 2) * (np.dot(um, um) + np.dot(vn, vn))\n",
    "            loss_sum += loss + regularization\n",
    "        arr[m] = loss_sum\n",
    "    # Total loss\n",
    "    total_loss = np.sum(arr)\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1,  Loss train: -604959.9362276391, Loss test: -31847.54811267625, rmse : 0.8655812392398312, rmse test: 0.8655364391261708\n",
      "epoch2,  Loss train: -586128.3516210453, Loss test: -30854.065659164433, rmse : 0.8546498049280299, rmse test: 0.854594826667045\n",
      "epoch3,  Loss train: -562890.6186185484, Loss test: -29630.9998433418, rmse : 0.8561907511148253, rmse test: 0.8562281819706602\n",
      "epoch4,  Loss train: -550606.2450890568, Loss test: -28982.576547057804, rmse : 0.8549080735053737, rmse test: 0.8548590739792917\n",
      "epoch5,  Loss train: -533251.6449867549, Loss test: -28068.767249079046, rmse : 0.8541953646974269, rmse test: 0.8541319774041506\n",
      "epoch6,  Loss train: -516656.70475172164, Loss test: -27197.527600578265, rmse : 0.8538555634286119, rmse test: 0.8538328491118862\n",
      "epoch7,  Loss train: -502269.46077137365, Loss test: -26439.043605573625, rmse : 0.8537280205517792, rmse test: 0.8537306661583373\n",
      "epoch8,  Loss train: -489253.8008164854, Loss test: -25751.207519847616, rmse : 0.8533114928270751, rmse test: 0.8532972658195525\n",
      "epoch9,  Loss train: -475976.16460769734, Loss test: -25050.672362042733, rmse : 0.8531188436579119, rmse test: 0.8530735917195233\n",
      "epoch10,  Loss train: -461313.64708622626, Loss test: -24280.15973166426, rmse : 0.85321719326373, rmse test: 0.8532010561998089\n",
      "epoch21,  Loss train: -386104.390724247, Loss test: -20319.31510584552, rmse : 0.8517632950672892, rmse test: 0.8518343854575398\n"
     ]
    }
   ],
   "source": [
    "M = len(data_by_user)\n",
    "N = len(data_by_movie)\n",
    "user_biases = np.zeros(M)\n",
    "movies_biases = np.zeros(N)\n",
    "K=10\n",
    "p=1/np.sqrt(K)\n",
    "user_vector = np.random.normal(0,p, (M,K))\n",
    "movie_vector = np.random.normal(0,p, (N,K))\n",
    "epoch = 21\n",
    "lamda = 0.001\n",
    "gamma = 0.004\n",
    "tau = 0.002\n",
    "\n",
    "Loss_train_list = []\n",
    "Loss_test_list = []\n",
    "mse_train_list = []\n",
    "mse_test_list = []\n",
    "\n",
    "for i in range (epoch):\n",
    "\n",
    "  user_biases, user_vector  = updtating_biases_vector_user(user_biases, lengths_user, lengths_user_incrementing, movies_biases, user_vector, movie_vector, lamda, gamma, tau, data_array_user)\n",
    "  movies_biases, movie_vector = updtating_biases_vector_movie(user_biases, movies_biases, user_vector, movie_vector, lamda, gamma, tau, lengths_movie, lengths_movie_incrementing, data_array_movie)\n",
    "  Loss_train = loss_calculate(data_array_user_train, lengths_user_train, lengths_us_train_incrementing, user_biases, movies_biases, user_vector, movie_vector, lamda, gamma,tau)\n",
    "  rmse = rmse_calculate(data_array_user_train, lengths_user_train, lengths_us_train_incrementing,  user_biases, movies_biases, user_vector, movie_vector)\n",
    "\n",
    "  mse_train_list.append(rmse)\n",
    "  Loss_train_list.append(Loss_train)\n",
    "  \n",
    "  Loss_test = loss_calculate(data_array_user_test, lengths_user_test, lengths_us_test_incrementing, user_biases, movies_biases, user_vector, movie_vector, lamda, gamma,tau)\n",
    "  rmse_test = rmse_calculate(data_array_user_test, lengths_user_test, lengths_us_test_incrementing, user_biases, movies_biases, user_vector, movie_vector)\n",
    " \n",
    "  mse_test_list.append(rmse_test)\n",
    "  Loss_test_list.append(Loss_test)\n",
    "\n",
    "  if i<10 or (i>10 and i%10==0):\n",
    "    print(f'epoch{i+1},  Loss train: {Loss_train}, Loss test: {Loss_test}, rmse : {rmse}, rmse test: {rmse_test}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "numbaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
